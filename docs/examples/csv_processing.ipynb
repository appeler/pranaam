{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Processing with Pranaam\n",
    "\n",
    "This notebook demonstrates how to process CSV files containing names and add religion predictions. This is useful for:\n",
    "\n",
    "- Processing employee databases\n",
    "- Analyzing customer lists\n",
    "- Research datasets\n",
    "- Survey responses\n",
    "\n",
    "We'll cover:\n",
    "1. Creating sample CSV data\n",
    "2. Reading and validating CSV files\n",
    "3. Processing names with error handling\n",
    "4. Saving enriched results\n",
    "5. Batch processing strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import pranaam\n",
    "\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Pranaam version: {pranaam.__version__ if hasattr(pranaam, '__version__') else 'latest'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Creating Sample CSV Data\n",
    "\n",
    "Let's start by creating a sample CSV file to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_csv(filename=\"sample_names.csv\"):\n",
    "    \"\"\"Create a sample CSV file for testing.\"\"\"\n",
    "    sample_data = pd.DataFrame({\n",
    "        \"id\": [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        \"full_name\": [\n",
    "            \"Shah Rukh Khan\",\n",
    "            \"Priya Sharma\",\n",
    "            \"Mohammed Ali\",\n",
    "            \"Raj Patel\",\n",
    "            \"Fatima Khan\",\n",
    "            \"John Smith\",\n",
    "            \"Deepika Padukone\",\n",
    "            \"Abdul Rahman\"\n",
    "        ],\n",
    "        \"department\": [\n",
    "            \"Engineering\", \"Marketing\", \"Finance\", \"HR\",\n",
    "            \"Sales\", \"IT\", \"Design\", \"Operations\"\n",
    "        ],\n",
    "        \"city\": [\n",
    "            \"Mumbai\", \"Delhi\", \"Bangalore\", \"Chennai\",\n",
    "            \"Pune\", \"Hyderabad\", \"Kolkata\", \"Ahmedabad\"\n",
    "        ],\n",
    "        \"salary\": [75000, 65000, 70000, 60000, 80000, 85000, 72000, 68000]\n",
    "    })\n",
    "    \n",
    "    sample_data.to_csv(filename, index=False)\n",
    "    print(f\"ðŸ“ Created sample file: {filename}\")\n",
    "    return sample_data\n",
    "\n",
    "# Create our sample data\n",
    "sample_df = create_sample_csv()\n",
    "print(\"\\nSample data:\")\n",
    "print(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“– CSV Processing Function\n",
    "\n",
    "Let's create a comprehensive function to process CSV files with names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_csv_with_pranaam(input_file, output_file, name_column, language=\"eng\", chunk_size=1000):\n",
    "    \"\"\"Process CSV file and add religion predictions.\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to input CSV file\n",
    "        output_file: Path to output CSV file\n",
    "        name_column: Name of column containing names\n",
    "        language: Language code ('eng' or 'hin')\n",
    "        chunk_size: Process in chunks for large files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validate input file\n",
    "    if not Path(input_file).exists():\n",
    "        print(f\"âŒ Error: Input file '{input_file}' not found\")\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        # Read CSV\n",
    "        print(f\"ðŸ“– Reading {input_file}...\")\n",
    "        df = pd.read_csv(input_file)\n",
    "        print(f\"   Found {len(df)} rows, {len(df.columns)} columns\")\n",
    "        \n",
    "        # Validate name column\n",
    "        if name_column not in df.columns:\n",
    "            print(f\"âŒ Error: Column '{name_column}' not found in CSV\")\n",
    "            print(f\"   Available columns: {list(df.columns)}\")\n",
    "            return False\n",
    "        \n",
    "        # Data quality checks\n",
    "        print(\"\\nðŸ” Data Quality Analysis:\")\n",
    "        total_rows = len(df)\n",
    "        missing_names = df[name_column].isna().sum()\n",
    "        empty_names = (df[name_column].str.strip() == \"\").sum() if not df[name_column].isna().all() else 0\n",
    "        \n",
    "        print(f\"   Total rows: {total_rows}\")\n",
    "        print(f\"   Missing names: {missing_names}\")\n",
    "        print(f\"   Empty names: {empty_names}\")\n",
    "        \n",
    "        # Clean data\n",
    "        if missing_names > 0 or empty_names > 0:\n",
    "            print(f\"   Removing {missing_names + empty_names} invalid rows...\")\n",
    "            df_clean = df.dropna(subset=[name_column])\n",
    "            df_clean = df_clean[df_clean[name_column].str.strip() != \"\"]\n",
    "        else:\n",
    "            df_clean = df.copy()\n",
    "            \n",
    "        valid_rows = len(df_clean)\n",
    "        print(f\"   Valid rows for processing: {valid_rows}\")\n",
    "        \n",
    "        if valid_rows == 0:\n",
    "            print(\"âŒ No valid names to process!\")\n",
    "            return False\n",
    "        \n",
    "        # Get predictions\n",
    "        print(f\"\\nðŸ”® Getting predictions for {valid_rows} names (language: {language})...\")\n",
    "        predictions = pranaam.pred_rel(df_clean[name_column], lang=language)\n",
    "        \n",
    "        # Rename prediction columns to avoid conflicts\n",
    "        predictions = predictions.rename(columns={\n",
    "            \"name\": name_column,\n",
    "            \"pred_label\": f\"{name_column}_religion\",\n",
    "            \"pred_prob_muslim\": f\"{name_column}_confidence_muslim\",\n",
    "        })\n",
    "        \n",
    "        # Merge predictions back\n",
    "        df_with_predictions = df_clean.merge(predictions, on=name_column, how=\"left\")\n",
    "        \n",
    "        # Add confidence score\n",
    "        conf_col = f\"{name_column}_confidence_muslim\"\n",
    "        df_with_predictions[f\"{name_column}_confidence\"] = df_with_predictions[conf_col].apply(\n",
    "            lambda x: max(x, 100 - x)\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        print(f\"ðŸ’¾ Saving results to {output_file}...\")\n",
    "        df_with_predictions.to_csv(output_file, index=False)\n",
    "        \n",
    "        # Generate summary\n",
    "        print(\"\\nðŸ“Š Processing Summary:\")\n",
    "        print(f\"   Input rows: {total_rows}\")\n",
    "        print(f\"   Valid names processed: {valid_rows}\")\n",
    "        print(f\"   Output rows: {len(df_with_predictions)}\")\n",
    "        \n",
    "        # Religion distribution\n",
    "        religion_counts = df_with_predictions[f\"{name_column}_religion\"].value_counts()\n",
    "        print(f\"   Religion predictions: {dict(religion_counts)}\")\n",
    "        \n",
    "        # Confidence analysis\n",
    "        high_conf_count = (df_with_predictions[f\"{name_column}_confidence\"] > 90).sum()\n",
    "        medium_conf_count = (\n",
    "            (df_with_predictions[f\"{name_column}_confidence\"] >= 70) &\n",
    "            (df_with_predictions[f\"{name_column}_confidence\"] <= 90)\n",
    "        ).sum()\n",
    "        low_conf_count = (df_with_predictions[f\"{name_column}_confidence\"] < 70).sum()\n",
    "        \n",
    "        print(\"   Confidence distribution:\")\n",
    "        print(f\"     High (>90%): {high_conf_count} predictions\")\n",
    "        print(f\"     Medium (70-90%): {medium_conf_count} predictions\")\n",
    "        print(f\"     Low (<70%): {low_conf_count} predictions\")\n",
    "        \n",
    "        print(f\"\\nâœ… Successfully processed {input_file} â†’ {output_file}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing file: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”„ Processing Our Sample Data\n",
    "\n",
    "Now let's process our sample CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the sample CSV\n",
    "input_file = \"sample_names.csv\"\n",
    "output_file = \"sample_names_with_predictions.csv\"\n",
    "\n",
    "success = process_csv_with_pranaam(\n",
    "    input_file=input_file,\n",
    "    output_file=output_file,\n",
    "    name_column=\"full_name\",\n",
    "    language=\"eng\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“‹ Examining the Results\n",
    "\n",
    "Let's load and examine the processed results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if success:\n",
    "    # Load the processed results\n",
    "    results_df = pd.read_csv(output_file)\n",
    "    \n",
    "    print(\"ðŸ“‹ Processed Results:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    print(\"\\nðŸ“ˆ New Columns Added:\")\n",
    "    new_columns = [col for col in results_df.columns if 'full_name' in col and col != 'full_name']\n",
    "    for col in new_columns:\n",
    "        print(f\"   â€¢ {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed analysis of predictions\n",
    "if success:\n",
    "    print(\"ðŸ” Detailed Prediction Analysis:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Name':<20} | {'Religion':<10} | {'Muslim %':<8} | {'Confidence':<10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for _, row in results_df.iterrows():\n",
    "        name = row['full_name']\n",
    "        religion = row['full_name_religion']\n",
    "        muslim_prob = row['full_name_confidence_muslim']\n",
    "        confidence = row['full_name_confidence']\n",
    "        \n",
    "        print(f\"{name:<20} | {religion:<10} | {muslim_prob:>6.1f}% | {confidence:>8.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Large File Processing Strategy\n",
    "\n",
    "For large CSV files, we need to process data in chunks to avoid memory issues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_large_csv(input_file, output_file, name_column, language=\"eng\", chunk_size=1000):\n",
    "    \"\"\"Process large CSV files in chunks to manage memory usage.\"\"\"\n",
    "    \n",
    "    print(f\"ðŸš€ Processing large CSV file: {input_file}\")\n",
    "    print(f\"   Chunk size: {chunk_size} rows\")\n",
    "    \n",
    "    # Get total row count first\n",
    "    total_rows = sum(1 for line in open(input_file)) - 1  # -1 for header\n",
    "    print(f\"   Total rows: {total_rows:,}\")\n",
    "    \n",
    "    processed_chunks = []\n",
    "    chunk_num = 0\n",
    "    \n",
    "    try:\n",
    "        # Process file in chunks\n",
    "        for chunk_df in pd.read_csv(input_file, chunksize=chunk_size):\n",
    "            chunk_num += 1\n",
    "            print(f\"\\nðŸ“¦ Processing chunk {chunk_num} ({len(chunk_df)} rows)...\")\n",
    "            \n",
    "            # Clean chunk\n",
    "            clean_chunk = chunk_df.dropna(subset=[name_column])\n",
    "            clean_chunk = clean_chunk[clean_chunk[name_column].str.strip() != \"\"]\n",
    "            \n",
    "            if len(clean_chunk) == 0:\n",
    "                print(\"   âš ï¸ No valid names in this chunk, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Get predictions for chunk\n",
    "            predictions = pranaam.pred_rel(clean_chunk[name_column], lang=language)\n",
    "            \n",
    "            # Rename columns\n",
    "            predictions = predictions.rename(columns={\n",
    "                \"name\": name_column,\n",
    "                \"pred_label\": f\"{name_column}_religion\",\n",
    "                \"pred_prob_muslim\": f\"{name_column}_confidence_muslim\",\n",
    "            })\n",
    "            \n",
    "            # Merge predictions\n",
    "            chunk_with_predictions = clean_chunk.merge(predictions, on=name_column, how=\"left\")\n",
    "            \n",
    "            # Add confidence score\n",
    "            conf_col = f\"{name_column}_confidence_muslim\"\n",
    "            chunk_with_predictions[f\"{name_column}_confidence\"] = chunk_with_predictions[conf_col].apply(\n",
    "                lambda x: max(x, 100 - x)\n",
    "            )\n",
    "            \n",
    "            processed_chunks.append(chunk_with_predictions)\n",
    "            print(f\"   âœ… Processed {len(chunk_with_predictions)} names\")\n",
    "        \n",
    "        # Combine all chunks\n",
    "        print(f\"\\nðŸ”— Combining {len(processed_chunks)} chunks...\")\n",
    "        final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "        \n",
    "        # Save results\n",
    "        print(f\"ðŸ’¾ Saving {len(final_df)} rows to {output_file}...\")\n",
    "        final_df.to_csv(output_file, index=False)\n",
    "        \n",
    "        print(\"\\nâœ… Large file processing completed!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing large file: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Demonstrate with our sample (simulating large file processing)\n",
    "print(\"Demonstrating large file processing strategy:\")\n",
    "large_file_success = process_large_csv(\n",
    "    input_file=\"sample_names.csv\",\n",
    "    output_file=\"sample_large_processed.csv\",\n",
    "    name_column=\"full_name\",\n",
    "    chunk_size=3  # Small chunk size for demo\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Validation and Quality Checks\n",
    "\n",
    "Let's create validation functions to ensure our processing worked correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_processed_csv(original_file, processed_file, name_column):\n",
    "    \"\"\"Validate that the processed CSV is correct.\"\"\"\n",
    "    \n",
    "    print(\"ðŸ” Validation Report:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Load both files\n",
    "    original_df = pd.read_csv(original_file)\n",
    "    processed_df = pd.read_csv(processed_file)\n",
    "    \n",
    "    # Basic checks\n",
    "    print(f\"Original file rows: {len(original_df)}\")\n",
    "    print(f\"Processed file rows: {len(processed_df)}\")\n",
    "    print(f\"Rows preserved: {len(processed_df) / len(original_df) * 100:.1f}%\")\n",
    "    \n",
    "    # Check for new columns\n",
    "    original_cols = set(original_df.columns)\n",
    "    processed_cols = set(processed_df.columns)\n",
    "    new_cols = processed_cols - original_cols\n",
    "    \n",
    "    print(f\"\\nNew columns added: {len(new_cols)}\")\n",
    "    for col in sorted(new_cols):\n",
    "        print(f\"  â€¢ {col}\")\n",
    "    \n",
    "    # Check predictions completeness\n",
    "    religion_col = f\"{name_column}_religion\"\n",
    "    if religion_col in processed_df.columns:\n",
    "        null_predictions = processed_df[religion_col].isna().sum()\n",
    "        print(\"\\nPrediction completeness:\")\n",
    "        print(f\"  Names with predictions: {len(processed_df) - null_predictions}\")\n",
    "        print(f\"  Names without predictions: {null_predictions}\")\n",
    "        \n",
    "        if null_predictions == 0:\n",
    "            print(\"  âœ… All names have predictions\")\n",
    "        else:\n",
    "            print(f\"  âš ï¸ {null_predictions} names missing predictions\")\n",
    "    \n",
    "    # Confidence distribution\n",
    "    conf_col = f\"{name_column}_confidence\"\n",
    "    if conf_col in processed_df.columns:\n",
    "        high_conf = (processed_df[conf_col] > 90).sum()\n",
    "        medium_conf = ((processed_df[conf_col] >= 70) & (processed_df[conf_col] <= 90)).sum()\n",
    "        low_conf = (processed_df[conf_col] < 70).sum()\n",
    "        \n",
    "        print(\"\\nConfidence distribution:\")\n",
    "        print(f\"  High confidence (>90%): {high_conf} ({high_conf/len(processed_df)*100:.1f}%)\")\n",
    "        print(f\"  Medium confidence (70-90%): {medium_conf} ({medium_conf/len(processed_df)*100:.1f}%)\")\n",
    "        print(f\"  Low confidence (<70%): {low_conf} ({low_conf/len(processed_df)*100:.1f}%)\")\n",
    "    \n",
    "    print(\"\\nâœ… Validation complete!\")\n",
    "\n",
    "# Validate our processed files\n",
    "if success:\n",
    "    validate_processed_csv(\"sample_names.csv\", \"sample_names_with_predictions.csv\", \"full_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§¹ Cleanup\n",
    "\n",
    "Let's clean up the demo files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Clean up demo files\n",
    "demo_files = [\n",
    "    \"sample_names.csv\",\n",
    "    \"sample_names_with_predictions.csv\",\n",
    "    \"sample_large_processed.csv\"\n",
    "]\n",
    "\n",
    "print(\"ðŸ§¹ Cleaning up demo files:\")\n",
    "for file in demo_files:\n",
    "    if os.path.exists(file):\n",
    "        os.remove(file)\n",
    "        print(f\"   âœ… Removed {file}\")\n",
    "    else:\n",
    "        print(f\"   â„¹ï¸ {file} not found\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ Demo cleanup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“ Command-Line Equivalent\n",
    "\n",
    "If you were to create a command-line script, here's what the usage would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This shows how you might structure a command-line interface\n",
    "def demonstrate_cli_usage():\n",
    "    print(\"ðŸ’» Command-Line Usage Examples:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    examples = [\n",
    "        {\n",
    "            \"description\": \"Basic CSV processing\",\n",
    "            \"command\": \"python csv_processor.py data.csv results.csv --name-column 'full_name'\"\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Process with Hindi names\",\n",
    "            \"command\": \"python csv_processor.py data.csv results.csv --name-column 'employee_name' --language hin\"\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Large file with custom chunk size\",\n",
    "            \"command\": \"python csv_processor.py large_data.csv results.csv --name-column 'name' --chunk-size 5000\"\n",
    "        },\n",
    "        {\n",
    "            \"description\": \"Create sample file for testing\",\n",
    "            \"command\": \"python csv_processor.py --create-sample\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for i, example in enumerate(examples, 1):\n",
    "        print(f\"\\n{i}. {example['description']}:\")\n",
    "        print(f\"   {example['command']}\")\n",
    "    \n",
    "    print(\"\\nðŸ“‹ Required Arguments:\")\n",
    "    print(\"   â€¢ input_file: Path to CSV file with names\")\n",
    "    print(\"   â€¢ output_file: Path for results CSV\")\n",
    "    print(\"   â€¢ --name-column: Column containing names\")\n",
    "    \n",
    "    print(\"\\nâš™ï¸ Optional Arguments:\")\n",
    "    print(\"   â€¢ --language: 'eng' or 'hin' (default: eng)\")\n",
    "    print(\"   â€¢ --chunk-size: Rows per chunk (default: 1000)\")\n",
    "    print(\"   â€¢ --create-sample: Generate test data\")\n",
    "\n",
    "demonstrate_cli_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "ðŸ“ **CSV Processing**: Pranaam seamlessly integrates with CSV workflows  \n",
    "ðŸ” **Data Validation**: Always validate input data and check for missing values  \n",
    "ðŸš€ **Chunk Processing**: Handle large files by processing in chunks  \n",
    "ðŸ“Š **Quality Metrics**: Monitor confidence scores to assess prediction quality  \n",
    "ðŸ”— **Column Naming**: Use consistent naming conventions for prediction columns  \n",
    "âœ… **Validation**: Always validate results to ensure processing completed correctly  \n",
    "\n",
    "## Best Practices for CSV Processing\n",
    "\n",
    "1. **Validate Input Data**\n",
    "   - Check file exists and is readable\n",
    "   - Verify required columns are present\n",
    "   - Handle missing or empty names gracefully\n",
    "\n",
    "2. **Memory Management**\n",
    "   - Use chunk processing for files > 100MB\n",
    "   - Choose appropriate chunk sizes (1000-5000 rows)\n",
    "   - Monitor memory usage during processing\n",
    "\n",
    "3. **Error Handling**\n",
    "   - Wrap processing in try-except blocks\n",
    "   - Log errors with sufficient detail\n",
    "   - Provide clear error messages to users\n",
    "\n",
    "4. **Output Quality**\n",
    "   - Use descriptive column names\n",
    "   - Include confidence scores\n",
    "   - Validate output completeness\n",
    "   - Save processing metadata\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **[Performance Benchmarks](performance_benchmarks.ipynb)**: Optimize for large-scale processing\n",
    "- **[Pandas Integration](pandas_integration.ipynb)**: Advanced DataFrame operations\n",
    "- **[Basic Usage](basic_usage.ipynb)**: Review fundamental concepts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}